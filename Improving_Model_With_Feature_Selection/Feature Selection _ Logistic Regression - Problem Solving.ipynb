{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Along :: Feature Selection and Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"spam\" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography... \n",
    "\n",
    "Our collection of spam e-mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word 'george' and the area code '650' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter. \n",
    "\n",
    "-  Number of Instances: 4601 (1813 Spam = 39.4%)\n",
    "-  Number of Attributes: 58 (57 continuous, 1 nominal class label)\n",
    "\n",
    " -  Attribute Information:\n",
    "\n",
    "    -  The last column of 'spambase.data' denotes whether the e-mail was \n",
    "       considered spam (1) or not (0)\n",
    "    \n",
    "    - 48 attributes are continuous real [0,100] numbers of type `word freq WORD` i.e. percentage of words in the e-mail that         match WORD\n",
    "\n",
    "    - 6 attributes are continuous real [0,100] numbers of type `char freq CHAR` i.e. percentage of characters in the e-mail           that match CHAR\n",
    "    \n",
    "    - 1 attribute is continuous real [1,...] numbers of type `capital run length average` i.e. average length of uninterrupted       sequences of capital letters\n",
    "\n",
    "    - 1 attribute is continuous integer [1,...] numbers of type `capital run length longest` i.e. length of longest                   uninterrupted sequence of capital letters\n",
    "\n",
    "    - 1 attribute is continuous integer [1,...] numbers of type `capital run length total` i.e. sum of length of uninterrupted       sequences of capital letters in the email\n",
    "\n",
    "    - 1 attribute is nominal {0,1} class  of type spam i.e  denotes whether the e-mail was considered spam (1) or not (0),  \n",
    "\n",
    "- Missing Attribute Values: None\n",
    "\n",
    "- Class Distribution:\n",
    "\tSpam\t  1813  (39.4%)\n",
    "\tNon-Spam  2788  (60.6%)\n",
    "\n",
    "\n",
    "\n",
    "You can read more about dataset [here](https://archive.ics.uci.edu/ml/datasets/spambase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the Spam data for the mini challenge\n",
    "#Target variable is the 57th column i.e spam, non-spam classes \n",
    "df = pd.read_csv('spambase.data.csv',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get an overview of your data by using info() and describe() functions of pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 58)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4601 entries, 0 to 4600\n",
      "Data columns (total 58 columns):\n",
      "0     4601 non-null float64\n",
      "1     4601 non-null float64\n",
      "2     4601 non-null float64\n",
      "3     4601 non-null float64\n",
      "4     4601 non-null float64\n",
      "5     4601 non-null float64\n",
      "6     4601 non-null float64\n",
      "7     4601 non-null float64\n",
      "8     4601 non-null float64\n",
      "9     4601 non-null float64\n",
      "10    4601 non-null float64\n",
      "11    4601 non-null float64\n",
      "12    4601 non-null float64\n",
      "13    4601 non-null float64\n",
      "14    4601 non-null float64\n",
      "15    4601 non-null float64\n",
      "16    4601 non-null float64\n",
      "17    4601 non-null float64\n",
      "18    4601 non-null float64\n",
      "19    4601 non-null float64\n",
      "20    4601 non-null float64\n",
      "21    4601 non-null float64\n",
      "22    4601 non-null float64\n",
      "23    4601 non-null float64\n",
      "24    4601 non-null float64\n",
      "25    4601 non-null float64\n",
      "26    4601 non-null float64\n",
      "27    4601 non-null float64\n",
      "28    4601 non-null float64\n",
      "29    4601 non-null float64\n",
      "30    4601 non-null float64\n",
      "31    4601 non-null float64\n",
      "32    4601 non-null float64\n",
      "33    4601 non-null float64\n",
      "34    4601 non-null float64\n",
      "35    4601 non-null float64\n",
      "36    4601 non-null float64\n",
      "37    4601 non-null float64\n",
      "38    4601 non-null float64\n",
      "39    4601 non-null float64\n",
      "40    4601 non-null float64\n",
      "41    4601 non-null float64\n",
      "42    4601 non-null float64\n",
      "43    4601 non-null float64\n",
      "44    4601 non-null float64\n",
      "45    4601 non-null float64\n",
      "46    4601 non-null float64\n",
      "47    4601 non-null float64\n",
      "48    4601 non-null float64\n",
      "49    4601 non-null float64\n",
      "50    4601 non-null float64\n",
      "51    4601 non-null float64\n",
      "52    4601 non-null float64\n",
      "53    4601 non-null float64\n",
      "54    4601 non-null float64\n",
      "55    4601 non-null int64\n",
      "56    4601 non-null int64\n",
      "57    4601 non-null int64\n",
      "dtypes: float64(55), int64(3)\n",
      "memory usage: 2.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.104553     0.213015     0.280656     0.065425     0.312223   \n",
       "std       0.305358     1.290575     0.504143     1.395151     0.672513   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.420000     0.000000     0.380000   \n",
       "max       4.540000    14.280000     5.100000    42.810000    10.000000   \n",
       "\n",
       "                5            6            7            8            9   ...  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000  ...   \n",
       "mean      0.095901     0.114208     0.105295     0.090067     0.239413  ...   \n",
       "std       0.273824     0.391441     0.401071     0.278616     0.644755  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.160000  ...   \n",
       "max       5.880000     7.270000    11.110000     5.260000    18.180000  ...   \n",
       "\n",
       "                48           49           50           51           52  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.038575     0.139030     0.016976     0.269071     0.075811   \n",
       "std       0.243471     0.270355     0.109394     0.815672     0.245882   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.065000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.188000     0.000000     0.315000     0.052000   \n",
       "max       4.385000     9.752000     4.081000    32.478000     6.003000   \n",
       "\n",
       "                53           54           55            56           57  \n",
       "count  4601.000000  4601.000000  4601.000000   4601.000000  4601.000000  \n",
       "mean      0.044238     5.191515    52.172789    283.289285     0.394045  \n",
       "std       0.429342    31.729449   194.891310    606.347851     0.488698  \n",
       "min       0.000000     1.000000     1.000000      1.000000     0.000000  \n",
       "25%       0.000000     1.588000     6.000000     35.000000     0.000000  \n",
       "50%       0.000000     2.276000    15.000000     95.000000     0.000000  \n",
       "75%       0.000000     3.706000    43.000000    266.000000     1.000000  \n",
       "max      19.829000  1102.500000  9989.000000  15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview of the data\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data into train and test set and fit the base logistic regression model on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dividing the dataset set in train and test set and apply base logistic model\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find out the accuracy , print out the Classification report and Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9319333816075308\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Classification report and Confusion Matrix.\n",
    "print(\"Accuracy on test data:\", lr.score(X_test,y_test))\n",
    "y_pred = lr.predict(X_test)\n",
    "#accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9319333816075308"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "517 34\n",
      "60 770\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\")#,confusion_matrix(y_test,y_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(tp, fp)\n",
    "print(fn, tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       804\n",
      "           1       0.94      0.90      0.92       577\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1381\n",
      "   macro avg       0.93      0.93      0.93      1381\n",
      "weighted avg       0.93      0.93      0.93      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report: \\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.16805450e-01, -1.28166822e-01,  1.14405099e-01,\n",
       "         7.34284240e-01,  5.99767114e-01,  5.23393968e-01,\n",
       "         2.04994572e+00,  4.44215763e-01,  1.14834883e+00,\n",
       "         8.35366878e-02, -3.54906079e-01, -1.09211759e-01,\n",
       "         3.41020364e-02,  1.57280831e-01,  1.33967166e+00,\n",
       "         1.06926351e+00,  9.85716797e-01,  7.79073045e-02,\n",
       "         1.02520248e-01,  9.26969623e-01,  2.48487177e-01,\n",
       "         2.49447422e-01,  2.14271849e+00,  3.07180333e-01,\n",
       "        -1.77622081e+00, -8.36888511e-01, -3.76150550e+00,\n",
       "         3.37513893e-01, -1.17319555e+00, -6.76766515e-01,\n",
       "        -1.36354100e-01, -6.88026680e-02, -1.17460011e+00,\n",
       "         1.46772108e-01, -9.10339625e-01,  9.83000129e-01,\n",
       "        -2.81212165e-01, -5.86646162e-01, -7.76659081e-01,\n",
       "        -1.77394328e-01, -1.16818857e+00, -1.46324346e+00,\n",
       "        -6.29666835e-01, -1.72698946e+00, -6.63654824e-01,\n",
       "        -1.31544276e+00, -5.85044405e-01, -1.43031573e+00,\n",
       "        -1.07896296e+00, -1.82356030e-01, -3.81539565e-01,\n",
       "         2.20069315e-01,  3.41121723e+00,  1.01926710e+00,\n",
       "        -1.14628133e-02,  6.33223031e-03,  8.58694318e-04]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.49485805])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Copy dataset df into df1 variable and apply correlation on df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy df in new variable df1\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. As we have learned  one of the assumptions of Logistic Regression model is that the independent features should not be correlated to each other(i.e Multicollinearity), So we have to find the features that have a correlation higher that 0.75 and remove the same so that the assumption for logistic regression model is satisfied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016759</td>\n",
       "      <td>0.065627</td>\n",
       "      <td>0.013273</td>\n",
       "      <td>0.023119</td>\n",
       "      <td>0.059674</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.041198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.021196</td>\n",
       "      <td>0.033301</td>\n",
       "      <td>0.058292</td>\n",
       "      <td>0.117419</td>\n",
       "      <td>0.008844</td>\n",
       "      <td>0.044491</td>\n",
       "      <td>0.061382</td>\n",
       "      <td>0.089165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016759</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.023760</td>\n",
       "      <td>0.024840</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>0.016280</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.032962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>0.049837</td>\n",
       "      <td>0.018527</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.009605</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.022680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.065627</td>\n",
       "      <td>0.033526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020246</td>\n",
       "      <td>0.077734</td>\n",
       "      <td>0.087564</td>\n",
       "      <td>0.036677</td>\n",
       "      <td>0.012003</td>\n",
       "      <td>0.093786</td>\n",
       "      <td>0.032075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026344</td>\n",
       "      <td>0.033213</td>\n",
       "      <td>0.016495</td>\n",
       "      <td>0.033120</td>\n",
       "      <td>0.108140</td>\n",
       "      <td>0.087618</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.097398</td>\n",
       "      <td>0.107463</td>\n",
       "      <td>0.070114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013273</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.020246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.010014</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>0.010268</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.012370</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.010862</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>0.021369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.023119</td>\n",
       "      <td>0.023760</td>\n",
       "      <td>0.077734</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.147336</td>\n",
       "      <td>0.029598</td>\n",
       "      <td>0.020823</td>\n",
       "      <td>0.034495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032005</td>\n",
       "      <td>0.032759</td>\n",
       "      <td>0.046361</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.041582</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.052662</td>\n",
       "      <td>0.052290</td>\n",
       "      <td>0.002492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.059674</td>\n",
       "      <td>0.024840</td>\n",
       "      <td>0.087564</td>\n",
       "      <td>0.010014</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061163</td>\n",
       "      <td>0.079561</td>\n",
       "      <td>0.117438</td>\n",
       "      <td>0.013897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031693</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>0.008705</td>\n",
       "      <td>0.015133</td>\n",
       "      <td>0.065043</td>\n",
       "      <td>0.105692</td>\n",
       "      <td>0.019894</td>\n",
       "      <td>0.010278</td>\n",
       "      <td>0.090172</td>\n",
       "      <td>0.082089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.007669</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>0.036677</td>\n",
       "      <td>0.019784</td>\n",
       "      <td>0.147336</td>\n",
       "      <td>0.061163</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044545</td>\n",
       "      <td>0.050786</td>\n",
       "      <td>0.056809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031408</td>\n",
       "      <td>0.033089</td>\n",
       "      <td>0.051885</td>\n",
       "      <td>0.027653</td>\n",
       "      <td>0.053706</td>\n",
       "      <td>0.070127</td>\n",
       "      <td>0.046612</td>\n",
       "      <td>0.041565</td>\n",
       "      <td>0.059677</td>\n",
       "      <td>0.008344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.016280</td>\n",
       "      <td>0.012003</td>\n",
       "      <td>0.010268</td>\n",
       "      <td>0.029598</td>\n",
       "      <td>0.079561</td>\n",
       "      <td>0.044545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.105302</td>\n",
       "      <td>0.083129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021224</td>\n",
       "      <td>0.027432</td>\n",
       "      <td>0.032494</td>\n",
       "      <td>0.019548</td>\n",
       "      <td>0.031454</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.008012</td>\n",
       "      <td>0.011254</td>\n",
       "      <td>0.037575</td>\n",
       "      <td>0.040252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.106263</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.093786</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.020823</td>\n",
       "      <td>0.117438</td>\n",
       "      <td>0.050786</td>\n",
       "      <td>0.105302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026017</td>\n",
       "      <td>0.014646</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.043639</td>\n",
       "      <td>0.149365</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.111308</td>\n",
       "      <td>0.189247</td>\n",
       "      <td>0.248724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.041198</td>\n",
       "      <td>0.032962</td>\n",
       "      <td>0.032075</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>0.034495</td>\n",
       "      <td>0.013897</td>\n",
       "      <td>0.056809</td>\n",
       "      <td>0.083129</td>\n",
       "      <td>0.130624</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016842</td>\n",
       "      <td>0.011945</td>\n",
       "      <td>0.003936</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.036737</td>\n",
       "      <td>0.075786</td>\n",
       "      <td>0.044830</td>\n",
       "      <td>0.073677</td>\n",
       "      <td>0.103308</td>\n",
       "      <td>0.087273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.188459</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.048254</td>\n",
       "      <td>0.012976</td>\n",
       "      <td>0.068382</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.159578</td>\n",
       "      <td>0.128495</td>\n",
       "      <td>0.137760</td>\n",
       "      <td>0.125319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>0.032410</td>\n",
       "      <td>0.055089</td>\n",
       "      <td>0.025183</td>\n",
       "      <td>0.024992</td>\n",
       "      <td>0.070227</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.029258</td>\n",
       "      <td>0.086791</td>\n",
       "      <td>0.115055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.105801</td>\n",
       "      <td>0.040398</td>\n",
       "      <td>0.083210</td>\n",
       "      <td>0.019221</td>\n",
       "      <td>0.066788</td>\n",
       "      <td>0.009264</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.030344</td>\n",
       "      <td>0.071157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032943</td>\n",
       "      <td>0.027711</td>\n",
       "      <td>0.030940</td>\n",
       "      <td>0.044966</td>\n",
       "      <td>0.013369</td>\n",
       "      <td>0.016723</td>\n",
       "      <td>0.030445</td>\n",
       "      <td>0.010002</td>\n",
       "      <td>0.021774</td>\n",
       "      <td>0.020076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.066438</td>\n",
       "      <td>0.018858</td>\n",
       "      <td>0.047593</td>\n",
       "      <td>0.013199</td>\n",
       "      <td>0.031126</td>\n",
       "      <td>0.077631</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>0.026274</td>\n",
       "      <td>0.034738</td>\n",
       "      <td>0.045737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019746</td>\n",
       "      <td>0.023445</td>\n",
       "      <td>0.051151</td>\n",
       "      <td>0.028283</td>\n",
       "      <td>0.040737</td>\n",
       "      <td>0.205905</td>\n",
       "      <td>0.014195</td>\n",
       "      <td>0.013446</td>\n",
       "      <td>0.041962</td>\n",
       "      <td>0.105150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.036780</td>\n",
       "      <td>0.009206</td>\n",
       "      <td>0.008552</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.022723</td>\n",
       "      <td>0.012426</td>\n",
       "      <td>0.066840</td>\n",
       "      <td>0.017901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016425</td>\n",
       "      <td>0.019045</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.014349</td>\n",
       "      <td>0.008499</td>\n",
       "      <td>0.080953</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.060993</td>\n",
       "      <td>0.169257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.028439</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>0.122113</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.056177</td>\n",
       "      <td>0.173066</td>\n",
       "      <td>0.042904</td>\n",
       "      <td>0.072782</td>\n",
       "      <td>0.238436</td>\n",
       "      <td>0.160543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020818</td>\n",
       "      <td>0.018277</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>0.018607</td>\n",
       "      <td>0.123854</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.151626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.059386</td>\n",
       "      <td>0.009117</td>\n",
       "      <td>0.063906</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>0.083024</td>\n",
       "      <td>0.019865</td>\n",
       "      <td>0.128436</td>\n",
       "      <td>0.051115</td>\n",
       "      <td>0.008269</td>\n",
       "      <td>0.025601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028084</td>\n",
       "      <td>0.026841</td>\n",
       "      <td>0.046578</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.104261</td>\n",
       "      <td>0.049953</td>\n",
       "      <td>0.035534</td>\n",
       "      <td>0.015036</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>0.003007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.081928</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.036262</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.143443</td>\n",
       "      <td>0.064137</td>\n",
       "      <td>0.187981</td>\n",
       "      <td>0.216422</td>\n",
       "      <td>0.158390</td>\n",
       "      <td>0.081363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029947</td>\n",
       "      <td>0.031542</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>0.036691</td>\n",
       "      <td>0.077049</td>\n",
       "      <td>0.098323</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.038126</td>\n",
       "      <td>0.062672</td>\n",
       "      <td>0.064261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.053324</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.121923</td>\n",
       "      <td>0.019391</td>\n",
       "      <td>0.062344</td>\n",
       "      <td>0.078350</td>\n",
       "      <td>0.122011</td>\n",
       "      <td>0.037738</td>\n",
       "      <td>0.098804</td>\n",
       "      <td>0.035977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017173</td>\n",
       "      <td>0.039519</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>0.017439</td>\n",
       "      <td>0.039350</td>\n",
       "      <td>0.063872</td>\n",
       "      <td>0.020978</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>0.046364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.128243</td>\n",
       "      <td>0.055476</td>\n",
       "      <td>0.139329</td>\n",
       "      <td>0.010834</td>\n",
       "      <td>0.098510</td>\n",
       "      <td>0.095505</td>\n",
       "      <td>0.111792</td>\n",
       "      <td>0.020641</td>\n",
       "      <td>0.039017</td>\n",
       "      <td>0.093509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040015</td>\n",
       "      <td>0.044314</td>\n",
       "      <td>0.128882</td>\n",
       "      <td>0.063826</td>\n",
       "      <td>0.153381</td>\n",
       "      <td>0.091470</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.030592</td>\n",
       "      <td>0.006530</td>\n",
       "      <td>0.007307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.021295</td>\n",
       "      <td>0.015806</td>\n",
       "      <td>0.031111</td>\n",
       "      <td>0.005381</td>\n",
       "      <td>0.031526</td>\n",
       "      <td>0.058979</td>\n",
       "      <td>0.046134</td>\n",
       "      <td>0.109163</td>\n",
       "      <td>0.123217</td>\n",
       "      <td>0.030859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018198</td>\n",
       "      <td>0.020851</td>\n",
       "      <td>0.021431</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>0.048350</td>\n",
       "      <td>0.034948</td>\n",
       "      <td>0.007214</td>\n",
       "      <td>0.067140</td>\n",
       "      <td>0.099463</td>\n",
       "      <td>0.075751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.197049</td>\n",
       "      <td>0.018191</td>\n",
       "      <td>0.156651</td>\n",
       "      <td>0.008176</td>\n",
       "      <td>0.136605</td>\n",
       "      <td>0.106833</td>\n",
       "      <td>0.130794</td>\n",
       "      <td>0.156905</td>\n",
       "      <td>0.159112</td>\n",
       "      <td>0.098072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050951</td>\n",
       "      <td>0.058660</td>\n",
       "      <td>0.085181</td>\n",
       "      <td>0.045469</td>\n",
       "      <td>0.084017</td>\n",
       "      <td>0.141649</td>\n",
       "      <td>0.004355</td>\n",
       "      <td>0.041066</td>\n",
       "      <td>0.085321</td>\n",
       "      <td>0.051797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.024349</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.035681</td>\n",
       "      <td>0.028102</td>\n",
       "      <td>0.020207</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.016192</td>\n",
       "      <td>0.019648</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013168</td>\n",
       "      <td>0.416608</td>\n",
       "      <td>0.046244</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.004838</td>\n",
       "      <td>0.011036</td>\n",
       "      <td>0.184428</td>\n",
       "      <td>0.021497</td>\n",
       "      <td>0.027775</td>\n",
       "      <td>0.103954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.134072</td>\n",
       "      <td>0.020502</td>\n",
       "      <td>0.123671</td>\n",
       "      <td>0.011368</td>\n",
       "      <td>0.070037</td>\n",
       "      <td>0.211455</td>\n",
       "      <td>0.064795</td>\n",
       "      <td>0.089226</td>\n",
       "      <td>0.126800</td>\n",
       "      <td>0.096809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030155</td>\n",
       "      <td>0.027362</td>\n",
       "      <td>0.033174</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.070103</td>\n",
       "      <td>0.310971</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.123036</td>\n",
       "      <td>0.165977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.188155</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.041145</td>\n",
       "      <td>0.035360</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.059329</td>\n",
       "      <td>0.030575</td>\n",
       "      <td>0.034127</td>\n",
       "      <td>0.099461</td>\n",
       "      <td>0.052129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019171</td>\n",
       "      <td>0.019139</td>\n",
       "      <td>0.033113</td>\n",
       "      <td>0.020798</td>\n",
       "      <td>0.051076</td>\n",
       "      <td>0.104691</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>0.044870</td>\n",
       "      <td>0.080993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.072504</td>\n",
       "      <td>0.043483</td>\n",
       "      <td>0.087924</td>\n",
       "      <td>0.015181</td>\n",
       "      <td>0.072502</td>\n",
       "      <td>0.084402</td>\n",
       "      <td>0.089494</td>\n",
       "      <td>0.053038</td>\n",
       "      <td>0.069931</td>\n",
       "      <td>0.033534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.029181</td>\n",
       "      <td>0.136979</td>\n",
       "      <td>0.039723</td>\n",
       "      <td>0.090862</td>\n",
       "      <td>0.086634</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>0.017285</td>\n",
       "      <td>0.051206</td>\n",
       "      <td>0.043267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.061686</td>\n",
       "      <td>0.038211</td>\n",
       "      <td>0.062459</td>\n",
       "      <td>0.013708</td>\n",
       "      <td>0.075456</td>\n",
       "      <td>0.087271</td>\n",
       "      <td>0.080330</td>\n",
       "      <td>0.041450</td>\n",
       "      <td>0.049775</td>\n",
       "      <td>0.013045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032860</td>\n",
       "      <td>0.013558</td>\n",
       "      <td>0.144771</td>\n",
       "      <td>0.064349</td>\n",
       "      <td>0.078367</td>\n",
       "      <td>0.081198</td>\n",
       "      <td>0.020691</td>\n",
       "      <td>0.024234</td>\n",
       "      <td>0.051806</td>\n",
       "      <td>0.059601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.066424</td>\n",
       "      <td>0.030307</td>\n",
       "      <td>0.108886</td>\n",
       "      <td>0.010684</td>\n",
       "      <td>0.088011</td>\n",
       "      <td>0.069051</td>\n",
       "      <td>0.065893</td>\n",
       "      <td>0.057189</td>\n",
       "      <td>0.064608</td>\n",
       "      <td>0.067817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.022724</td>\n",
       "      <td>0.028748</td>\n",
       "      <td>0.017676</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.068728</td>\n",
       "      <td>0.020561</td>\n",
       "      <td>0.025504</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.096548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.048680</td>\n",
       "      <td>0.029221</td>\n",
       "      <td>0.050648</td>\n",
       "      <td>0.010368</td>\n",
       "      <td>0.061501</td>\n",
       "      <td>0.066223</td>\n",
       "      <td>0.066947</td>\n",
       "      <td>0.049988</td>\n",
       "      <td>0.056764</td>\n",
       "      <td>0.019356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.025020</td>\n",
       "      <td>0.313835</td>\n",
       "      <td>0.031979</td>\n",
       "      <td>0.063495</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.011438</td>\n",
       "      <td>0.013757</td>\n",
       "      <td>0.038772</td>\n",
       "      <td>0.067596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.041251</td>\n",
       "      <td>0.021940</td>\n",
       "      <td>0.057726</td>\n",
       "      <td>0.007798</td>\n",
       "      <td>0.032048</td>\n",
       "      <td>0.048673</td>\n",
       "      <td>0.048482</td>\n",
       "      <td>0.037047</td>\n",
       "      <td>0.044840</td>\n",
       "      <td>0.026903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003303</td>\n",
       "      <td>0.018502</td>\n",
       "      <td>0.158593</td>\n",
       "      <td>0.006575</td>\n",
       "      <td>0.042330</td>\n",
       "      <td>0.050231</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0.014936</td>\n",
       "      <td>0.034733</td>\n",
       "      <td>0.056628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.052799</td>\n",
       "      <td>0.027508</td>\n",
       "      <td>0.032547</td>\n",
       "      <td>0.010476</td>\n",
       "      <td>0.052066</td>\n",
       "      <td>0.048127</td>\n",
       "      <td>0.058101</td>\n",
       "      <td>0.043405</td>\n",
       "      <td>0.043643</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.019845</td>\n",
       "      <td>0.224192</td>\n",
       "      <td>0.004667</td>\n",
       "      <td>0.061694</td>\n",
       "      <td>0.065475</td>\n",
       "      <td>0.082593</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>0.039001</td>\n",
       "      <td>0.064115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.039066</td>\n",
       "      <td>0.018097</td>\n",
       "      <td>0.038927</td>\n",
       "      <td>0.007529</td>\n",
       "      <td>0.042535</td>\n",
       "      <td>0.046383</td>\n",
       "      <td>0.046280</td>\n",
       "      <td>0.035816</td>\n",
       "      <td>0.040158</td>\n",
       "      <td>0.024423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013711</td>\n",
       "      <td>0.016280</td>\n",
       "      <td>0.233392</td>\n",
       "      <td>0.010718</td>\n",
       "      <td>0.045273</td>\n",
       "      <td>0.047475</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.010897</td>\n",
       "      <td>0.027449</td>\n",
       "      <td>0.045923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.032058</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.061870</td>\n",
       "      <td>0.006717</td>\n",
       "      <td>0.026748</td>\n",
       "      <td>0.036835</td>\n",
       "      <td>0.040538</td>\n",
       "      <td>0.034276</td>\n",
       "      <td>0.033984</td>\n",
       "      <td>0.015137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011731</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.304679</td>\n",
       "      <td>0.013805</td>\n",
       "      <td>0.041529</td>\n",
       "      <td>0.043484</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.010498</td>\n",
       "      <td>0.027732</td>\n",
       "      <td>0.046796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.041014</td>\n",
       "      <td>0.024903</td>\n",
       "      <td>0.054759</td>\n",
       "      <td>0.008075</td>\n",
       "      <td>0.031998</td>\n",
       "      <td>0.034164</td>\n",
       "      <td>0.041372</td>\n",
       "      <td>0.039220</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>0.005691</td>\n",
       "      <td>0.028655</td>\n",
       "      <td>0.113105</td>\n",
       "      <td>0.048493</td>\n",
       "      <td>0.048101</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>0.015509</td>\n",
       "      <td>0.025919</td>\n",
       "      <td>0.006919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.027690</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>0.061706</td>\n",
       "      <td>0.006729</td>\n",
       "      <td>0.026960</td>\n",
       "      <td>0.037315</td>\n",
       "      <td>0.040910</td>\n",
       "      <td>0.034811</td>\n",
       "      <td>0.033601</td>\n",
       "      <td>0.014434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012065</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.303606</td>\n",
       "      <td>0.013687</td>\n",
       "      <td>0.038626</td>\n",
       "      <td>0.039844</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.024532</td>\n",
       "      <td>0.044529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.044954</td>\n",
       "      <td>0.024058</td>\n",
       "      <td>0.048335</td>\n",
       "      <td>0.006122</td>\n",
       "      <td>0.049732</td>\n",
       "      <td>0.054315</td>\n",
       "      <td>0.053202</td>\n",
       "      <td>0.035174</td>\n",
       "      <td>0.041847</td>\n",
       "      <td>0.020092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.021592</td>\n",
       "      <td>0.200713</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.048822</td>\n",
       "      <td>0.048947</td>\n",
       "      <td>0.009650</td>\n",
       "      <td>0.013707</td>\n",
       "      <td>0.030236</td>\n",
       "      <td>0.045963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.054673</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>0.046504</td>\n",
       "      <td>0.006515</td>\n",
       "      <td>0.048844</td>\n",
       "      <td>0.052819</td>\n",
       "      <td>0.053978</td>\n",
       "      <td>0.033747</td>\n",
       "      <td>0.056270</td>\n",
       "      <td>0.016955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010395</td>\n",
       "      <td>0.018947</td>\n",
       "      <td>0.245454</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.060379</td>\n",
       "      <td>0.057933</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.019185</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>0.045792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.057312</td>\n",
       "      <td>0.024013</td>\n",
       "      <td>0.067015</td>\n",
       "      <td>0.007761</td>\n",
       "      <td>0.072599</td>\n",
       "      <td>0.057465</td>\n",
       "      <td>0.052035</td>\n",
       "      <td>0.017466</td>\n",
       "      <td>0.033244</td>\n",
       "      <td>0.004944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048099</td>\n",
       "      <td>0.052138</td>\n",
       "      <td>0.107674</td>\n",
       "      <td>0.073010</td>\n",
       "      <td>0.054578</td>\n",
       "      <td>0.063895</td>\n",
       "      <td>0.022637</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.033204</td>\n",
       "      <td>0.003490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.007960</td>\n",
       "      <td>0.008922</td>\n",
       "      <td>0.032407</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.130812</td>\n",
       "      <td>0.017918</td>\n",
       "      <td>0.014781</td>\n",
       "      <td>0.012119</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.017950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017660</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>0.010430</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.015126</td>\n",
       "      <td>0.012909</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.013897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.011134</td>\n",
       "      <td>0.019124</td>\n",
       "      <td>0.014809</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.042044</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.046978</td>\n",
       "      <td>0.030392</td>\n",
       "      <td>0.040844</td>\n",
       "      <td>0.016091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009546</td>\n",
       "      <td>0.034492</td>\n",
       "      <td>0.107928</td>\n",
       "      <td>0.038474</td>\n",
       "      <td>0.024846</td>\n",
       "      <td>0.044513</td>\n",
       "      <td>0.011326</td>\n",
       "      <td>0.014032</td>\n",
       "      <td>0.029229</td>\n",
       "      <td>0.049256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.036095</td>\n",
       "      <td>0.014821</td>\n",
       "      <td>0.047066</td>\n",
       "      <td>0.007643</td>\n",
       "      <td>0.021442</td>\n",
       "      <td>0.029866</td>\n",
       "      <td>0.022121</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.009867</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015802</td>\n",
       "      <td>0.018693</td>\n",
       "      <td>0.268701</td>\n",
       "      <td>0.014065</td>\n",
       "      <td>0.032509</td>\n",
       "      <td>0.016724</td>\n",
       "      <td>0.010661</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.028806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.009703</td>\n",
       "      <td>0.015420</td>\n",
       "      <td>0.030956</td>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.047505</td>\n",
       "      <td>0.029457</td>\n",
       "      <td>0.033120</td>\n",
       "      <td>0.003884</td>\n",
       "      <td>0.035177</td>\n",
       "      <td>0.025084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.053034</td>\n",
       "      <td>0.017584</td>\n",
       "      <td>0.034408</td>\n",
       "      <td>0.025911</td>\n",
       "      <td>0.036610</td>\n",
       "      <td>0.011755</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>0.023658</td>\n",
       "      <td>0.026373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.026070</td>\n",
       "      <td>0.025177</td>\n",
       "      <td>0.005811</td>\n",
       "      <td>0.008095</td>\n",
       "      <td>0.115041</td>\n",
       "      <td>0.054812</td>\n",
       "      <td>0.049664</td>\n",
       "      <td>0.043626</td>\n",
       "      <td>0.048223</td>\n",
       "      <td>0.054467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.007817</td>\n",
       "      <td>0.013082</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.038094</td>\n",
       "      <td>0.043653</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>0.017899</td>\n",
       "      <td>0.034585</td>\n",
       "      <td>0.056511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.024292</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.044325</td>\n",
       "      <td>0.009268</td>\n",
       "      <td>0.048879</td>\n",
       "      <td>0.030616</td>\n",
       "      <td>0.049079</td>\n",
       "      <td>0.004542</td>\n",
       "      <td>0.034190</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006238</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.060568</td>\n",
       "      <td>0.115548</td>\n",
       "      <td>0.049362</td>\n",
       "      <td>0.054698</td>\n",
       "      <td>0.013925</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>0.017279</td>\n",
       "      <td>0.036529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.022116</td>\n",
       "      <td>0.019739</td>\n",
       "      <td>0.053464</td>\n",
       "      <td>0.005933</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>0.028826</td>\n",
       "      <td>0.034461</td>\n",
       "      <td>0.030134</td>\n",
       "      <td>0.035159</td>\n",
       "      <td>0.026654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.007257</td>\n",
       "      <td>0.003203</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>0.033837</td>\n",
       "      <td>0.036241</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.013157</td>\n",
       "      <td>0.025918</td>\n",
       "      <td>0.040661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.037105</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.050664</td>\n",
       "      <td>0.012957</td>\n",
       "      <td>0.042336</td>\n",
       "      <td>0.053637</td>\n",
       "      <td>0.050811</td>\n",
       "      <td>0.002423</td>\n",
       "      <td>0.075558</td>\n",
       "      <td>0.032065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005125</td>\n",
       "      <td>0.024698</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.008838</td>\n",
       "      <td>0.067569</td>\n",
       "      <td>0.049367</td>\n",
       "      <td>0.023878</td>\n",
       "      <td>0.026979</td>\n",
       "      <td>0.051858</td>\n",
       "      <td>0.095444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.034056</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.056655</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>0.077986</td>\n",
       "      <td>0.033046</td>\n",
       "      <td>0.056166</td>\n",
       "      <td>0.037916</td>\n",
       "      <td>0.056817</td>\n",
       "      <td>0.030326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015719</td>\n",
       "      <td>0.015382</td>\n",
       "      <td>0.014763</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.028845</td>\n",
       "      <td>0.050109</td>\n",
       "      <td>0.015040</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.033365</td>\n",
       "      <td>0.046371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.009818</td>\n",
       "      <td>0.029339</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.014343</td>\n",
       "      <td>0.017512</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>0.007521</td>\n",
       "      <td>0.015546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006349</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.003085</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.017679</td>\n",
       "      <td>0.018549</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.006465</td>\n",
       "      <td>0.010154</td>\n",
       "      <td>0.005158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.015747</td>\n",
       "      <td>0.026344</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.032005</td>\n",
       "      <td>0.031693</td>\n",
       "      <td>0.031408</td>\n",
       "      <td>0.021224</td>\n",
       "      <td>0.026017</td>\n",
       "      <td>0.016842</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>0.006310</td>\n",
       "      <td>0.026576</td>\n",
       "      <td>0.030751</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.008114</td>\n",
       "      <td>0.016894</td>\n",
       "      <td>0.010033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>0.033213</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.032759</td>\n",
       "      <td>0.019119</td>\n",
       "      <td>0.033089</td>\n",
       "      <td>0.027432</td>\n",
       "      <td>0.014646</td>\n",
       "      <td>0.011945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049124</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>0.006392</td>\n",
       "      <td>0.055057</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.040829</td>\n",
       "      <td>0.055298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.021196</td>\n",
       "      <td>0.049837</td>\n",
       "      <td>0.016495</td>\n",
       "      <td>0.012370</td>\n",
       "      <td>0.046361</td>\n",
       "      <td>0.008705</td>\n",
       "      <td>0.051885</td>\n",
       "      <td>0.032494</td>\n",
       "      <td>0.031003</td>\n",
       "      <td>0.003936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>0.049124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022316</td>\n",
       "      <td>0.030354</td>\n",
       "      <td>0.044722</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>0.034365</td>\n",
       "      <td>0.370963</td>\n",
       "      <td>0.112209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.033301</td>\n",
       "      <td>0.018527</td>\n",
       "      <td>0.033120</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>0.015133</td>\n",
       "      <td>0.027653</td>\n",
       "      <td>0.019548</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006310</td>\n",
       "      <td>0.009070</td>\n",
       "      <td>0.022316</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031769</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.008180</td>\n",
       "      <td>0.013994</td>\n",
       "      <td>0.006016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.058292</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.108140</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.065043</td>\n",
       "      <td>0.053706</td>\n",
       "      <td>0.031454</td>\n",
       "      <td>0.043639</td>\n",
       "      <td>0.036737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026576</td>\n",
       "      <td>0.020539</td>\n",
       "      <td>0.030354</td>\n",
       "      <td>0.031769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142913</td>\n",
       "      <td>0.020924</td>\n",
       "      <td>0.054308</td>\n",
       "      <td>0.077392</td>\n",
       "      <td>0.036321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.117419</td>\n",
       "      <td>0.009605</td>\n",
       "      <td>0.087618</td>\n",
       "      <td>0.010862</td>\n",
       "      <td>0.041582</td>\n",
       "      <td>0.105692</td>\n",
       "      <td>0.070127</td>\n",
       "      <td>0.057910</td>\n",
       "      <td>0.149365</td>\n",
       "      <td>0.075786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030751</td>\n",
       "      <td>0.006392</td>\n",
       "      <td>0.044722</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.142913</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.079998</td>\n",
       "      <td>0.183144</td>\n",
       "      <td>0.201948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.008844</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.019894</td>\n",
       "      <td>0.046612</td>\n",
       "      <td>0.008012</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.044830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008575</td>\n",
       "      <td>0.055057</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>0.006863</td>\n",
       "      <td>0.020924</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013497</td>\n",
       "      <td>0.061657</td>\n",
       "      <td>0.042568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.044491</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.097398</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.052662</td>\n",
       "      <td>0.010278</td>\n",
       "      <td>0.041565</td>\n",
       "      <td>0.011254</td>\n",
       "      <td>0.111308</td>\n",
       "      <td>0.073677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008114</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.034365</td>\n",
       "      <td>0.008180</td>\n",
       "      <td>0.054308</td>\n",
       "      <td>0.079998</td>\n",
       "      <td>0.013497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.492638</td>\n",
       "      <td>0.162314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.061382</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.107463</td>\n",
       "      <td>0.022081</td>\n",
       "      <td>0.052290</td>\n",
       "      <td>0.090172</td>\n",
       "      <td>0.059677</td>\n",
       "      <td>0.037575</td>\n",
       "      <td>0.189247</td>\n",
       "      <td>0.103308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016894</td>\n",
       "      <td>0.040829</td>\n",
       "      <td>0.370963</td>\n",
       "      <td>0.013994</td>\n",
       "      <td>0.077392</td>\n",
       "      <td>0.183144</td>\n",
       "      <td>0.061657</td>\n",
       "      <td>0.492638</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.475486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.089165</td>\n",
       "      <td>0.022680</td>\n",
       "      <td>0.070114</td>\n",
       "      <td>0.021369</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.082089</td>\n",
       "      <td>0.008344</td>\n",
       "      <td>0.040252</td>\n",
       "      <td>0.248724</td>\n",
       "      <td>0.087273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010033</td>\n",
       "      <td>0.055298</td>\n",
       "      <td>0.112209</td>\n",
       "      <td>0.006016</td>\n",
       "      <td>0.036321</td>\n",
       "      <td>0.201948</td>\n",
       "      <td>0.042568</td>\n",
       "      <td>0.162314</td>\n",
       "      <td>0.475486</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000  0.016759  0.065627  0.013273  0.023119  0.059674  0.007669   \n",
       "1   0.016759  1.000000  0.033526  0.006923  0.023760  0.024840  0.003918   \n",
       "2   0.065627  0.033526  1.000000  0.020246  0.077734  0.087564  0.036677   \n",
       "3   0.013273  0.006923  0.020246  1.000000  0.003238  0.010014  0.019784   \n",
       "4   0.023119  0.023760  0.077734  0.003238  1.000000  0.054054  0.147336   \n",
       "5   0.059674  0.024840  0.087564  0.010014  0.054054  1.000000  0.061163   \n",
       "6   0.007669  0.003918  0.036677  0.019784  0.147336  0.061163  1.000000   \n",
       "7   0.003950  0.016280  0.012003  0.010268  0.029598  0.079561  0.044545   \n",
       "8   0.106263  0.003826  0.093786  0.002454  0.020823  0.117438  0.050786   \n",
       "9   0.041198  0.032962  0.032075  0.004947  0.034495  0.013897  0.056809   \n",
       "10  0.188459  0.006864  0.048254  0.012976  0.068382  0.053900  0.159578   \n",
       "11  0.105801  0.040398  0.083210  0.019221  0.066788  0.009264  0.001461   \n",
       "12  0.066438  0.018858  0.047593  0.013199  0.031126  0.077631  0.013295   \n",
       "13  0.036780  0.009206  0.008552  0.012008  0.003445  0.009673  0.022723   \n",
       "14  0.028439  0.005330  0.122113  0.002707  0.056177  0.173066  0.042904   \n",
       "15  0.059386  0.009117  0.063906  0.007432  0.083024  0.019865  0.128436   \n",
       "16  0.081928  0.018370  0.036262  0.003470  0.143443  0.064137  0.187981   \n",
       "17  0.053324  0.033500  0.121923  0.019391  0.062344  0.078350  0.122011   \n",
       "18  0.128243  0.055476  0.139329  0.010834  0.098510  0.095505  0.111792   \n",
       "19  0.021295  0.015806  0.031111  0.005381  0.031526  0.058979  0.046134   \n",
       "20  0.197049  0.018191  0.156651  0.008176  0.136605  0.106833  0.130794   \n",
       "21  0.024349  0.008850  0.035681  0.028102  0.020207  0.007956  0.002093   \n",
       "22  0.134072  0.020502  0.123671  0.011368  0.070037  0.211455  0.064795   \n",
       "23  0.188155  0.001984  0.041145  0.035360  0.000039  0.059329  0.030575   \n",
       "24  0.072504  0.043483  0.087924  0.015181  0.072502  0.084402  0.089494   \n",
       "25  0.061686  0.038211  0.062459  0.013708  0.075456  0.087271  0.080330   \n",
       "26  0.066424  0.030307  0.108886  0.010684  0.088011  0.069051  0.065893   \n",
       "27  0.048680  0.029221  0.050648  0.010368  0.061501  0.066223  0.066947   \n",
       "28  0.041251  0.021940  0.057726  0.007798  0.032048  0.048673  0.048482   \n",
       "29  0.052799  0.027508  0.032547  0.010476  0.052066  0.048127  0.058101   \n",
       "30  0.039066  0.018097  0.038927  0.007529  0.042535  0.046383  0.046280   \n",
       "31  0.032058  0.003326  0.061870  0.006717  0.026748  0.036835  0.040538   \n",
       "32  0.041014  0.024903  0.054759  0.008075  0.031998  0.034164  0.041372   \n",
       "33  0.027690  0.004303  0.061706  0.006729  0.026960  0.037315  0.040910   \n",
       "34  0.044954  0.024058  0.048335  0.006122  0.049732  0.054315  0.053202   \n",
       "35  0.054673  0.028198  0.046504  0.006515  0.048844  0.052819  0.053978   \n",
       "36  0.057312  0.024013  0.067015  0.007761  0.072599  0.057465  0.052035   \n",
       "37  0.007960  0.008922  0.032407  0.002669  0.130812  0.017918  0.014781   \n",
       "38  0.011134  0.019124  0.014809  0.004602  0.042044  0.047619  0.046978   \n",
       "39  0.036095  0.014821  0.047066  0.007643  0.021442  0.029866  0.022121   \n",
       "40  0.009703  0.015420  0.030956  0.005670  0.047505  0.029457  0.033120   \n",
       "41  0.026070  0.025177  0.005811  0.008095  0.115041  0.054812  0.049664   \n",
       "42  0.024292  0.002370  0.044325  0.009268  0.048879  0.030616  0.049079   \n",
       "43  0.022116  0.019739  0.053464  0.005933  0.015234  0.028826  0.034461   \n",
       "44  0.037105  0.016418  0.050664  0.012957  0.042336  0.053637  0.050811   \n",
       "45  0.034056  0.023858  0.056655  0.009181  0.077986  0.033046  0.056166   \n",
       "46  0.000953  0.009818  0.029339  0.003348  0.026900  0.014343  0.017512   \n",
       "47  0.017755  0.015747  0.026344  0.001924  0.032005  0.031693  0.031408   \n",
       "48  0.026505  0.007282  0.033213  0.000591  0.032759  0.019119  0.033089   \n",
       "49  0.021196  0.049837  0.016495  0.012370  0.046361  0.008705  0.051885   \n",
       "50  0.033301  0.018527  0.033120  0.007148  0.026390  0.015133  0.027653   \n",
       "51  0.058292  0.014461  0.108140  0.003138  0.025509  0.065043  0.053706   \n",
       "52  0.117419  0.009605  0.087618  0.010862  0.041582  0.105692  0.070127   \n",
       "53  0.008844  0.001946  0.003336  0.000298  0.002016  0.019894  0.046612   \n",
       "54  0.044491  0.002083  0.097398  0.005260  0.052662  0.010278  0.041565   \n",
       "55  0.061382  0.000271  0.107463  0.022081  0.052290  0.090172  0.059677   \n",
       "56  0.089165  0.022680  0.070114  0.021369  0.002492  0.082089  0.008344   \n",
       "\n",
       "          7         8         9   ...        47        48        49        50  \\\n",
       "0   0.003950  0.106263  0.041198  ...  0.017755  0.026505  0.021196  0.033301   \n",
       "1   0.016280  0.003826  0.032962  ...  0.015747  0.007282  0.049837  0.018527   \n",
       "2   0.012003  0.093786  0.032075  ...  0.026344  0.033213  0.016495  0.033120   \n",
       "3   0.010268  0.002454  0.004947  ...  0.001924  0.000591  0.012370  0.007148   \n",
       "4   0.029598  0.020823  0.034495  ...  0.032005  0.032759  0.046361  0.026390   \n",
       "5   0.079561  0.117438  0.013897  ...  0.031693  0.019119  0.008705  0.015133   \n",
       "6   0.044545  0.050786  0.056809  ...  0.031408  0.033089  0.051885  0.027653   \n",
       "7   1.000000  0.105302  0.083129  ...  0.021224  0.027432  0.032494  0.019548   \n",
       "8   0.105302  1.000000  0.130624  ...  0.026017  0.014646  0.031003  0.013601   \n",
       "9   0.083129  0.130624  1.000000  ...  0.016842  0.011945  0.003936  0.007357   \n",
       "10  0.128495  0.137760  0.125319  ...  0.022689  0.032410  0.055089  0.025183   \n",
       "11  0.002973  0.030344  0.071157  ...  0.032943  0.027711  0.030940  0.044966   \n",
       "12  0.026274  0.034738  0.045737  ...  0.019746  0.023445  0.051151  0.028283   \n",
       "13  0.012426  0.066840  0.017901  ...  0.016425  0.019045  0.005804  0.014349   \n",
       "14  0.072782  0.238436  0.160543  ...  0.020818  0.018277  0.002551  0.003111   \n",
       "15  0.051115  0.008269  0.025601  ...  0.028084  0.026841  0.046578  0.029560   \n",
       "16  0.216422  0.158390  0.081363  ...  0.029947  0.031542  0.035897  0.036691   \n",
       "17  0.037738  0.098804  0.035977  ...  0.017173  0.039519  0.035897  0.017439   \n",
       "18  0.020641  0.039017  0.093509  ...  0.040015  0.044314  0.128882  0.063826   \n",
       "19  0.109163  0.123217  0.030859  ...  0.018198  0.020851  0.021431  0.012071   \n",
       "20  0.156905  0.159112  0.098072  ...  0.050951  0.058660  0.085181  0.045469   \n",
       "21  0.016192  0.019648  0.008200  ...  0.013168  0.416608  0.046244  0.001137   \n",
       "22  0.089226  0.126800  0.096809  ...  0.030155  0.027362  0.033174  0.000467   \n",
       "23  0.034127  0.099461  0.052129  ...  0.019171  0.019139  0.033113  0.020798   \n",
       "24  0.053038  0.069931  0.033534  ...  0.002739  0.029181  0.136979  0.039723   \n",
       "25  0.041450  0.049775  0.013045  ...  0.032860  0.013558  0.144771  0.064349   \n",
       "26  0.057189  0.064608  0.067817  ...  0.007194  0.022724  0.028748  0.017676   \n",
       "27  0.049988  0.056764  0.019356  ...  0.004689  0.025020  0.313835  0.031979   \n",
       "28  0.037047  0.044840  0.026903  ...  0.003303  0.018502  0.158593  0.006575   \n",
       "29  0.043405  0.043643  0.008677  ...  0.005172  0.019845  0.224192  0.004667   \n",
       "30  0.035816  0.040158  0.024423  ...  0.013711  0.016280  0.233392  0.010718   \n",
       "31  0.034276  0.033984  0.015137  ...  0.011731  0.008853  0.304679  0.013805   \n",
       "32  0.039220  0.014403  0.035366  ...  0.004533  0.005691  0.028655  0.113105   \n",
       "33  0.034811  0.033601  0.014434  ...  0.012065  0.009290  0.303606  0.013687   \n",
       "34  0.035174  0.041847  0.020092  ...  0.002160  0.021592  0.200713  0.034435   \n",
       "35  0.033747  0.056270  0.016955  ...  0.010395  0.018947  0.245454  0.001017   \n",
       "36  0.017466  0.033244  0.004944  ...  0.048099  0.052138  0.107674  0.073010   \n",
       "37  0.012119  0.002216  0.017950  ...  0.017660  0.007886  0.010430  0.001767   \n",
       "38  0.030392  0.040844  0.016091  ...  0.009546  0.034492  0.107928  0.038474   \n",
       "39  0.005988  0.009867  0.004163  ...  0.015802  0.018693  0.268701  0.014065   \n",
       "40  0.003884  0.035177  0.025084  ...  0.001284  0.053034  0.017584  0.034408   \n",
       "41  0.043626  0.048223  0.054467  ...  0.000529  0.007817  0.013082  0.011091   \n",
       "42  0.004542  0.034190  0.023200  ...  0.006238  0.015385  0.060568  0.115548   \n",
       "43  0.030134  0.035159  0.026654  ...  0.000808  0.007257  0.003203  0.010733   \n",
       "44  0.002423  0.075558  0.032065  ...  0.005125  0.024698  0.001413  0.008838   \n",
       "45  0.037916  0.056817  0.030326  ...  0.015719  0.015382  0.014763  0.003168   \n",
       "46  0.006397  0.007521  0.015546  ...  0.006349  0.000995  0.003085  0.004592   \n",
       "47  0.021224  0.026017  0.016842  ...  1.000000  0.002290  0.012795  0.006310   \n",
       "48  0.027432  0.014646  0.011945  ...  0.002290  1.000000  0.049124  0.009070   \n",
       "49  0.032494  0.031003  0.003936  ...  0.012795  0.049124  1.000000  0.022316   \n",
       "50  0.019548  0.013601  0.007357  ...  0.006310  0.009070  0.022316  1.000000   \n",
       "51  0.031454  0.043639  0.036737  ...  0.026576  0.020539  0.030354  0.031769   \n",
       "52  0.057910  0.149365  0.075786  ...  0.030751  0.006392  0.044722  0.026400   \n",
       "53  0.008012  0.000522  0.044830  ...  0.008575  0.055057  0.023322  0.006863   \n",
       "54  0.011254  0.111308  0.073677  ...  0.008114  0.003443  0.034365  0.008180   \n",
       "55  0.037575  0.189247  0.103308  ...  0.016894  0.040829  0.370963  0.013994   \n",
       "56  0.040252  0.248724  0.087273  ...  0.010033  0.055298  0.112209  0.006016   \n",
       "\n",
       "          51        52        53        54        55        56  \n",
       "0   0.058292  0.117419  0.008844  0.044491  0.061382  0.089165  \n",
       "1   0.014461  0.009605  0.001946  0.002083  0.000271  0.022680  \n",
       "2   0.108140  0.087618  0.003336  0.097398  0.107463  0.070114  \n",
       "3   0.003138  0.010862  0.000298  0.005260  0.022081  0.021369  \n",
       "4   0.025509  0.041582  0.002016  0.052662  0.052290  0.002492  \n",
       "5   0.065043  0.105692  0.019894  0.010278  0.090172  0.082089  \n",
       "6   0.053706  0.070127  0.046612  0.041565  0.059677  0.008344  \n",
       "7   0.031454  0.057910  0.008012  0.011254  0.037575  0.040252  \n",
       "8   0.043639  0.149365  0.000522  0.111308  0.189247  0.248724  \n",
       "9   0.036737  0.075786  0.044830  0.073677  0.103308  0.087273  \n",
       "10  0.024992  0.070227  0.001126  0.029258  0.086791  0.115055  \n",
       "11  0.013369  0.016723  0.030445  0.010002  0.021774  0.020076  \n",
       "12  0.040737  0.205905  0.014195  0.013446  0.041962  0.105150  \n",
       "13  0.008499  0.080953  0.006545  0.003023  0.060993  0.169257  \n",
       "14  0.018607  0.123854  0.005446  0.017383  0.213992  0.151626  \n",
       "15  0.104261  0.049953  0.035534  0.015036  0.026528  0.003007  \n",
       "16  0.077049  0.098323  0.000466  0.038126  0.062672  0.064261  \n",
       "17  0.039350  0.063872  0.020978  0.007979  0.075122  0.046364  \n",
       "18  0.153381  0.091470  0.002434  0.030592  0.006530  0.007307  \n",
       "19  0.048350  0.034948  0.007214  0.067140  0.099463  0.075751  \n",
       "20  0.084017  0.141649  0.004355  0.041066  0.085321  0.051797  \n",
       "21  0.004838  0.011036  0.184428  0.021497  0.027775  0.103954  \n",
       "22  0.070103  0.310971  0.020140  0.008372  0.123036  0.165977  \n",
       "23  0.051076  0.104691  0.000703  0.007681  0.044870  0.080993  \n",
       "24  0.090862  0.086634  0.058780  0.017285  0.051206  0.043267  \n",
       "25  0.078367  0.081198  0.020691  0.024234  0.051806  0.059601  \n",
       "26  0.067500  0.068728  0.020561  0.025504  0.054400  0.096548  \n",
       "27  0.063495  0.061441  0.011438  0.013757  0.038772  0.067596  \n",
       "28  0.042330  0.050231  0.002076  0.014936  0.034733  0.056628  \n",
       "29  0.061694  0.065475  0.082593  0.016599  0.039001  0.064115  \n",
       "30  0.045273  0.047475  0.000225  0.010897  0.027449  0.045923  \n",
       "31  0.041529  0.043484  0.010735  0.010498  0.027732  0.046796  \n",
       "32  0.048493  0.048101  0.009928  0.015509  0.025919  0.006919  \n",
       "33  0.038626  0.039844  0.010635  0.002404  0.024532  0.044529  \n",
       "34  0.048822  0.048947  0.009650  0.013707  0.030236  0.045963  \n",
       "35  0.060379  0.057933  0.006452  0.019185  0.038100  0.045792  \n",
       "36  0.054578  0.063895  0.022637  0.014423  0.033204  0.003490  \n",
       "37  0.015126  0.012909  0.003627  0.006012  0.009487  0.013897  \n",
       "38  0.024846  0.044513  0.011326  0.014032  0.029229  0.049256  \n",
       "39  0.032509  0.016724  0.010661  0.003945  0.004835  0.028806  \n",
       "40  0.025911  0.036610  0.011755  0.008895  0.023658  0.026373  \n",
       "41  0.038094  0.043653  0.003873  0.017899  0.034585  0.056511  \n",
       "42  0.049362  0.054698  0.013925  0.017681  0.017279  0.036529  \n",
       "43  0.033837  0.036241  0.001167  0.013157  0.025918  0.040661  \n",
       "44  0.067569  0.049367  0.023878  0.026979  0.051858  0.095444  \n",
       "45  0.028845  0.050109  0.015040  0.017408  0.033365  0.046371  \n",
       "46  0.017679  0.018549  0.000308  0.006465  0.010154  0.005158  \n",
       "47  0.026576  0.030751  0.008575  0.008114  0.016894  0.010033  \n",
       "48  0.020539  0.006392  0.055057  0.003443  0.040829  0.055298  \n",
       "49  0.030354  0.044722  0.023322  0.034365  0.370963  0.112209  \n",
       "50  0.031769  0.026400  0.006863  0.008180  0.013994  0.006016  \n",
       "51  1.000000  0.142913  0.020924  0.054308  0.077392  0.036321  \n",
       "52  0.142913  1.000000  0.012613  0.079998  0.183144  0.201948  \n",
       "53  0.020924  0.012613  1.000000  0.013497  0.061657  0.042568  \n",
       "54  0.054308  0.079998  0.013497  1.000000  0.492638  0.162314  \n",
       "55  0.077392  0.183144  0.061657  0.492638  1.000000  0.475486  \n",
       "56  0.036321  0.201948  0.042568  0.162314  0.475486  1.000000  \n",
       "\n",
       "[57 rows x 57 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.drop(57,1).corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Correlated features above 0.75 and then apply logistic model\n",
    "corr_matrix = df1.drop(57,1).corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to be dropped: \n",
      "[33, 39]\n"
     ]
    }
   ],
   "source": [
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n",
    "print(\"Columns to be dropped: \")\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(to_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Split the  new subset of the  data acquired by feature selection into train and test set and fit the logistic regression model on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=101, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the new subset of data and fit the logistic model on training data\n",
    "X = df1.iloc[:,:-1]\n",
    "y = df1.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 42)\n",
    "lr = LogisticRegression(random_state=101)\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Find out the accuracy , print out the Classification report and Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9304851556842868\n",
      "========================================\n",
      "Confusion Matrix:\n",
      "517 36\n",
      "60 768\n",
      "========================================\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       804\n",
      "           1       0.93      0.90      0.92       577\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1381\n",
      "   macro avg       0.93      0.93      0.93      1381\n",
      "weighted avg       0.93      0.93      0.93      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Classification report and Confusion Matrix for new data\n",
    "print(\"Accuracy on test data:\", lr.score(X_test,y_test))\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"==\"*20)\n",
    "print(\"Confusion Matrix:\")#,confusion_matrix(y_test,y_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(tp, fp)\n",
    "print(fn, tn)\n",
    "print(\"==\"*20)\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.16312961e-01, -1.22685386e-01,  1.09229931e-01,\n",
       "         7.34777380e-01,  6.11079101e-01,  5.46169663e-01,\n",
       "         1.95734252e+00,  4.46812689e-01,  1.09448415e+00,\n",
       "         8.41690171e-02, -3.69402937e-01, -1.19030303e-01,\n",
       "         3.85172140e-02,  1.52881485e-01,  1.31317730e+00,\n",
       "         1.05423405e+00,  9.70568403e-01,  9.14412771e-02,\n",
       "         9.95073451e-02,  9.28763362e-01,  2.48941194e-01,\n",
       "         2.52146762e-01,  2.12427485e+00,  2.89857438e-01,\n",
       "        -1.74008716e+00, -8.26775316e-01, -3.81041612e+00,\n",
       "         3.51311910e-01, -1.18688124e+00, -6.82835117e-01,\n",
       "        -2.71736695e-01, -8.12329143e-02, -1.15157973e+00,\n",
       "        -9.06714014e-01,  9.29402313e-01, -2.69508399e-01,\n",
       "        -5.58584648e-01, -7.71684461e-01, -1.10916437e+00,\n",
       "        -1.44185421e+00, -6.36660371e-01, -1.74713780e+00,\n",
       "        -6.60213632e-01, -1.28250737e+00, -5.60665124e-01,\n",
       "        -1.41402657e+00, -1.07372006e+00, -1.66838694e-01,\n",
       "        -3.86902330e-01,  2.17592041e-01,  3.42981521e+00,\n",
       "         1.00034629e+00, -1.13693891e-02,  6.31362313e-03,\n",
       "         8.35345357e-04]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.47098721])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. After keeping highly correlated features, there is not much change in the score. Lets apply another feature selection technique(Chi Squared test) to see whether we can increase our score. Find the optimum number of features using Chi Square and fit the logistic model on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 20 , score= 0.9029688631426502\n",
      "For no of features= 25 , score= 0.9152787834902245\n",
      "For no of features= 30 , score= 0.9131064446053584\n",
      "For no of features= 35 , score= 0.9196234612599565\n",
      "For no of features= 40 , score= 0.9254163649529327\n",
      "For no of features= 50 , score= 0.9254163649529327\n",
      "For no of features= 55 , score= 0.9304851556842868\n",
      "High Score is: 0.9304851556842868 with features= 55\n"
     ]
    }
   ],
   "source": [
    "# Apply Chi Square and fit the logistic model on train data use df dataset\n",
    "nof_list=[20,25,30,35,40,50,55]\n",
    "high_score=0\n",
    "nof=0\n",
    "\n",
    "for n in nof_list:\n",
    "    test = SelectKBest(score_func=chi2 , k= n )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 42)\n",
    "    X_train = test.fit_transform(X_train,y_train)\n",
    "    X_test = test.transform(X_test)\n",
    "    \n",
    "    model = LogisticRegression(random_state=101)\n",
    "    model.fit(X_train,y_train)\n",
    "    print(\"For no of features=\",n,\", score=\", model.score(X_test,y_test))\n",
    "    if model.score(X_test,y_test)>high_score:\n",
    "        high_score=model.score(X_test,y_test)\n",
    "        nof=n \n",
    "print(\"High Score is:\",high_score, \"with features=\",nof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Find out the accuracy , print out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9304851556842868\n",
      "========================================\n",
      "Confusion Matrix:\n",
      "517 36\n",
      "60 768\n",
      "========================================\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94       804\n",
      "           1       0.93      0.90      0.92       577\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1381\n",
      "   macro avg       0.93      0.93      0.93      1381\n",
      "weighted avg       0.93      0.93      0.93      1381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Classification report and Confusion Matrix for new data\n",
    "print(\"Accuracy on test data:\", lr.score(X_test,y_test))\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"==\"*20)\n",
    "print(\"Confusion Matrix:\")#,confusion_matrix(y_test,y_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(tp, fp)\n",
    "print(fn, tn)\n",
    "print(\"==\"*20)\n",
    "print(\"Classification Report: \\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "# y_pred = lr.predict(X_test)\n",
    "# print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Using chi squared test there is no change in the score and the optimum features that we got is 55. Now lets see if we can increase our score using another feature selection technique called Anova.Find the optimum number of features using Anova and fit the logistic model on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 20 , score= 0.889210716871832\n",
      "For no of features= 25 , score= 0.9015206372194062\n",
      "For no of features= 30 , score= 0.9145546705286025\n",
      "For no of features= 35 , score= 0.9203475742215785\n",
      "For no of features= 40 , score= 0.9217958001448225\n",
      "For no of features= 50 , score= 0.9261404779145547\n",
      "For no of features= 55 , score= 0.9304851556842868\n",
      "High Score is: 0.9304851556842868 with features= 55\n",
      "Confusion Matrix: \n",
      " [[768  36]\n",
      " [ 60 517]]\n"
     ]
    }
   ],
   "source": [
    "# Apply Anova and fit the logistic model on train data use df dataset\n",
    "nof_list=[20,25,30,35,40,50,55]\n",
    "high_score=0\n",
    "nof=0\n",
    "\n",
    "for n in nof_list:\n",
    "    test = SelectKBest(score_func=f_classif , k= n )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "    X_train = test.fit_transform(X_train,y_train)\n",
    "    X_test = test.transform(X_test)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    print(\"For no of features=\",n,\", score=\", model.score(X_test,y_test))\n",
    "\n",
    "    if model.score(X_test,y_test)>high_score:\n",
    "        high_score=model.score(X_test,y_test)\n",
    "        nof=n \n",
    "print(\"High Score is:\",high_score, \"with features=\",nof)\n",
    "\n",
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Find out the accuracy , print out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[768  36]\n",
      " [ 60 517]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Unfortunately Anova also couldn't give us a better score . Let's finally attempt PCA on train data and find if it helps in  giving a better model by reducing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no of features= 20 , score= 0.9036929761042722\n",
      "For no of features= 25 , score= 0.9109341057204924\n",
      "For no of features= 30 , score= 0.9138305575669804\n",
      "For no of features= 35 , score= 0.9239681390296887\n",
      "For no of features= 40 , score= 0.9188993482983345\n",
      "For no of features= 50 , score= 0.9275887038377987\n",
      "For no of features= 55 , score= 0.9312092686459088\n",
      "High Score is: 0.9312092686459088 with features= 55\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA and fit the logistic model on train data use df dataset\n",
    "nof_list=[20,25,30,35,40,50,55]\n",
    "high_score=0\n",
    "nof=0\n",
    "\n",
    "for n in nof_list:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state = 42)\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    logistic = LogisticRegression(random_state=101)\n",
    "    logistic.fit(X_train, y_train)\n",
    "    print(\"For no of features=\",n,\", score=\", logistic.score(X_test,y_test))\n",
    "    \n",
    "    if logistic.score(X_test,y_test)>high_score:\n",
    "        high_score=logistic.score(X_test,y_test)\n",
    "        nof=n \n",
    "print(\"High Score is:\",high_score, \"with features=\",nof)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Find out the accuracy , print out the Confusion Matrix.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[124 680]\n",
      " [239 338]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy , print out the Confusion Matrix \n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. You can also compare your predicted values and observed values by printing out values of logistic.predict(X_test[]) and  y_test[].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for 10 observation:     [0 0 0 1 0 1 0 0 0 0]\n",
      "Actual values for 10 observation:  [0 0 0 1 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Compare observed value and Predicted value\n",
    "print(\"Prediction for 10 observation:    \",logistic.predict(X_test[0:10]))\n",
    "print(\"Actual values for 10 observation: \",y_test[0:10].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
